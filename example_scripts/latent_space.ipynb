{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from functools import partial\n",
    "from src.model.diffusion_transformer import DiffusionTransformer\n",
    "from train.schedulers import GaussianDiffusion\n",
    "import datetime\n",
    "from flax.jax_utils import replicate\n",
    "from functools import reduce\n",
    "\n",
    "from configs.global_config import global_config\n",
    "from configs.dit_config import dit_config\n",
    "global_config.dropout_flag = False \n",
    "\n",
    "### Load embedding \n",
    "with open('../embeddings/protoken_emb.pkl', 'rb') as f:\n",
    "    protoken_emb = jnp.array(pkl.load(f), dtype=jnp.float32)\n",
    "with open('../embeddings/aatype_emb.pkl', 'rb') as f:\n",
    "    aatype_emb = jnp.array(pkl.load(f), dtype=jnp.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### constants\n",
    "NRES = 512 ### test in ProToken paper (2024.07) Nres = 256\n",
    "NSAMPLE_PER_DEVICE = 8\n",
    "DIM_EMB = protoken_emb.shape[-1] + aatype_emb.shape[-1] # 40 # 32 + 8\n",
    "NDEVICES = len(jax.devices())\n",
    "\n",
    "BATCH_SIZE = NSAMPLE_PER_DEVICE * NDEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### function utils \n",
    "\n",
    "def split_multiple_rng_keys(rng_key, num_keys):\n",
    "    rng_keys = jax.random.split(rng_key, num_keys + 1)\n",
    "    return rng_keys[:-1], rng_keys[-1]\n",
    "\n",
    "def flatten_list_of_dicts(list_of_dicts):\n",
    "    ### [{a: [1,2,3,4]}] -> [{a:1}, {a:2}, {a:3}, {a:4}]\n",
    "    flattened_lists = [[{k: v[i] for k, v in d.items()} \n",
    "                        for i in range(len(next(iter(d.values()))))] for d in list_of_dicts]\n",
    "    return reduce(lambda x, y: x+y, flattened_lists, [])\n",
    "\n",
    "def protoken_emb_distance_fn(x, y):\n",
    "    x_ = x / (jnp.linalg.norm(x, axis=-1, keepdims=True) + 1e-6)\n",
    "    y_ = y / (jnp.linalg.norm(y, axis=-1, keepdims=True) + 1e-6)\n",
    "    \n",
    "    return -jnp.sum(x_ * y_, axis=-1)\n",
    "\n",
    "def aatype_emb_distance_fn(x, y):\n",
    "    return jnp.sum((x - y) ** 2, axis=-1)\n",
    "\n",
    "def aatype_index_to_resname(aatype_index):\n",
    "    restypes = [\n",
    "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P',\n",
    "        'S', 'T', 'W', 'Y', 'V'\n",
    "    ]\n",
    "    \n",
    "    return \"\".join([restypes[int(i)] for i in aatype_index])\n",
    "\n",
    "def resname_to_aatype_index(resnames):\n",
    "    restypes = [\n",
    "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P',\n",
    "        'S', 'T', 'W', 'Y', 'V'\n",
    "    ]\n",
    "    return np.array([restypes.index(a) for a in resnames], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load model & params \n",
    "dit_model = DiffusionTransformer(\n",
    "    config=dit_config, global_config=global_config\n",
    ")\n",
    "num_diffusion_timesteps = 500\n",
    "scheduler = GaussianDiffusion(num_diffusion_timesteps=num_diffusion_timesteps)\n",
    "\n",
    "#### rng keys\n",
    "rng_key = jax.random.PRNGKey(8888)\n",
    "np.random.seed(7777)\n",
    "\n",
    "##### load params\n",
    "ckpt_path = '../ckpts/PT_DiT_params_1000000.pkl'\n",
    "with open(ckpt_path, \"rb\") as f:\n",
    "    params = pkl.load(f)\n",
    "    params = jax.tree_util.tree_map(lambda x: jnp.array(x), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### main inference functions\n",
    "jit_apply_fn = jax.jit(dit_model.apply)\n",
    "infer_protuple = True\n",
    "\n",
    "def index_from_embedding(x):\n",
    "    # x: (B, Nres, Nemb)\n",
    "    protoken_indexes = \\\n",
    "        jnp.argmin(protoken_emb_distance_fn(x[..., None, :protoken_emb.shape[-1]], \n",
    "                                            protoken_emb[None, None, ...]), axis=-1)\n",
    "    ret = {'protoken_indexes': protoken_indexes}\n",
    "    if bool(infer_protuple):\n",
    "        aatype_indexes = \\\n",
    "            jnp.argmin(aatype_emb_distance_fn(x[..., None, protoken_emb.shape[-1]:], \n",
    "                                                aatype_emb[None, None, ...]), axis=-1)\n",
    "        ret.update({'aatype_indexes': aatype_indexes})\n",
    "        \n",
    "    return ret            \n",
    "    \n",
    "pjit_index_from_embedding = jax.pmap(jax.jit(index_from_embedding), axis_name=\"i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PF-ODE Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffrax import diffeqsolve, Dopri5, ODETerm, SaveAt, PIDController\n",
    "\n",
    "def ode_drift(x, t, seq_mask, residue_index):\n",
    "    t_arr = jnp.full((x.shape[0],), t)\n",
    "    indicator = params['params']['protoken_indicator']\n",
    "    if bool(infer_protuple):\n",
    "        indicator = jnp.concatenate([indicator, params['params']['aatype_indicator']], \n",
    "                                    axis=-1)\n",
    "    eps_prime = jit_apply_fn({'params': params['params']['model']}, x + indicator[None, ...], \n",
    "                            seq_mask, t_arr, tokens_rope_index=residue_index)\n",
    "\n",
    "    beta_t = scheduler.betas[jnp.int32(t)]\n",
    "    sqrt_one_minus_alphas_cumprod_t = scheduler.sqrt_one_minus_alphas_cumprod[jnp.int32(t)]\n",
    "    \n",
    "    return 0.5 * beta_t * (-x + 1.0 / sqrt_one_minus_alphas_cumprod_t * eps_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtol, atol, method = 1e-5, 1e-5, \"RK45\"\n",
    "\n",
    "def solve_ode(t_0, t_1, dt0, x_0, seq_mask, residue_index):\n",
    "    term = ODETerm(lambda t, y, args: jax.jit(ode_drift)(y, t, seq_mask, residue_index))\n",
    "    solver = Dopri5()\n",
    "    stepsize_controller = PIDController(rtol=rtol, atol=atol)\n",
    "\n",
    "    sol = diffeqsolve(term, solver, t0=t_0, t1=t_1, y0=x_0, dt0=dt0,\n",
    "                        stepsize_controller=stepsize_controller, max_steps=65536)\n",
    "    \n",
    "    return sol.ys[-1]\n",
    "\n",
    "pjit_solve_ode = jax.pmap(jax.jit(solve_ode), axis_name='i', in_axes=(None, None, None, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: MurD close -> open\n",
    "\n",
    "### obatin ProTokens\n",
    "working_dir = '../'\n",
    "os.system(f'''export PYTHONPATH={working_dir}/PROTOKEN\n",
    "          python {working_dir}/PROTOKEN/scripts/infer_batch.py\\\n",
    "            --encoder_config {working_dir}/PROTOKEN/config/encoder.yaml\\\n",
    "            --decoder_config {working_dir}/PROTOKEN/config/decoder.yaml\\\n",
    "            --vq_config {working_dir}/PROTOKEN/config/vq.yaml\\\n",
    "            --pdb_dir_path {working_dir}/example_scripts/results/latent_interpolation/raw_pdbs\\\n",
    "            --save_dir_path {working_dir}/example_scripts/results/latent_interpolation/raw_pdbs\\\n",
    "            --load_ckpt_path {working_dir}/ckpts/protoken_params_100000.pkl''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "protoken_dir = 'results/latent_interpolation/raw_pdbs/stage_1/generator_inputs'\n",
    "pdb_list = ['MurD_close.pdb', ] * (BATCH_SIZE // 2) + \\\n",
    "           ['MurD_open.pdb', ] * (BATCH_SIZE // 2)\n",
    "\n",
    "data_dicts = []\n",
    "for pdb_file in pdb_list:\n",
    "    protoken_file = os.path.join(protoken_dir, pdb_file.replace('.pdb', '.pkl'))\n",
    "    with open(protoken_file, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    \n",
    "    seq_len = data['seq_len']\n",
    "    embedding = np.concatenate(\n",
    "        [protoken_emb[data['protokens'].astype(np.int32)], \n",
    "         aatype_emb[data['aatype'].astype(np.int32)]], axis=-1\n",
    "    )\n",
    "    embedding = np.pad(embedding, ((0, NRES - seq_len), (0,0)))\n",
    "    data_dicts.append(\n",
    "        {'embedding': embedding, \n",
    "         'seq_mask': np.pad(data['seq_mask'], (0, NRES - seq_len)).astype(np.bool_), \n",
    "         'residue_index': np.pad(data['residue_index'], (0, NRES - seq_len)).astype(np.int32),}\n",
    "    )\n",
    "    \n",
    "data_dict = {k: np.stack([d[k] for d in data_dicts], axis=0) for k in data_dicts[0].keys()}\n",
    "\n",
    "### for pmap: reshape inputs\n",
    "reshape_func = lambda x:x.reshape(NDEVICES, x.shape[0]//NDEVICES, *x.shape[1:])\n",
    "data_dict = jax.tree_util.tree_map(reshape_func, data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward PF-ODE: data -> latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = data_dict['embedding']\n",
    "xT = pjit_solve_ode(0, scheduler.num_timesteps, 1.0, x0, data_dict['seq_mask'], data_dict['residue_index'])\n",
    "xT_np = np.array(xT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_arr = np.linspace(0, 1, BATCH_SIZE)\n",
    "\n",
    "xT = xT.reshape(BATCH_SIZE, NRES, DIM_EMB)\n",
    "xT_A, xT_B = xT[0], xT[-1] ## end-point\n",
    "xT_interpolation = []\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    xT_interpolation.append(\n",
    "        ((1.0 - lambda_arr[i]) * xT_A + lambda_arr[i] * xT_B)\n",
    "    )\n",
    "xT_interpolation = jnp.array(xT_interpolation).reshape(NDEVICES, NSAMPLE_PER_DEVICE, NRES, DIM_EMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward PF-ODE: latent -> data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_interpolation = pjit_solve_ode(scheduler.num_timesteps, 0, -1.0, \n",
    "                                  xT_interpolation, data_dict['seq_mask'], data_dict['residue_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = {'embedding': x0_interpolation, 'seq_mask': data_dict['seq_mask'], 'residue_index': data_dict['residue_index']}\n",
    "ret.update(pjit_index_from_embedding(ret['embedding']))\n",
    "\n",
    "ret = jax.tree_util.tree_map(lambda x: np.array(x.reshape(BATCH_SIZE, *x.shape[2:])).tolist(), ret)\n",
    "with open('results/latent_interpolation/result.pkl', 'wb') as f:\n",
    "    pkl.dump(ret, f)\n",
    "    \n",
    "ret_ = flatten_list_of_dicts([ret])\n",
    "with open('results/latent_interpolation/result_flatten.pkl', 'wb') as f:\n",
    "    pkl.dump(ret_, f)\n",
    "    \n",
    "os.makedirs('results/latent_interpolation/interpolation_pdb', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '../'\n",
    "\n",
    "os.system(f'''export PYTHONPATH={working_dir}/PROTOKEN\n",
    "          python {working_dir}/PROTOKEN/scripts/decode_structure.py\\\n",
    "                --decoder_config {working_dir}/PROTOKEN/config/decoder.yaml\\\n",
    "                --vq_config {working_dir}/PROTOKEN/config/vq.yaml\\\n",
    "                --input_path results/latent_interpolation/result_flatten.pkl\\\n",
    "                --output_dir results/latent_interpolation/interpolation_pdb\\\n",
    "                --load_ckpt_path {working_dir}/ckpts/protoken_params_100000.pkl\\\n",
    "                --padding_len {NRES}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Directed Evolution (Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: cas12f\n",
    "\n",
    "### obatin ProTokens\n",
    "working_dir = '../'\n",
    "os.system(f'''export PYTHONPATH={working_dir}/PROTOKEN\n",
    "          python {working_dir}/PROTOKEN/scripts/infer_batch.py\\\n",
    "            --encoder_config {working_dir}/PROTOKEN/config/encoder.yaml\\\n",
    "            --decoder_config {working_dir}/PROTOKEN/config/decoder.yaml\\\n",
    "            --vq_config {working_dir}/PROTOKEN/config/vq.yaml\\\n",
    "            --pdb_dir_path {working_dir}/example_scripts/results/latent_directed_evo/cas12f\\\n",
    "            --save_dir_path {working_dir}/example_scripts/results/latent_directed_evo/cas12f\\\n",
    "            --load_ckpt_path {working_dir}/ckpts/protoken_params_100000.pkl''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, seq_fitness_dict = [], {}\n",
    "ordered_seq_neg, ordered_seq_pos = [], []\n",
    "### load dataset (variant seqs/fitness)\n",
    "with open(f'results/latent_directed_evo/cas12f/cas12f_negative_sequences_with_fitness_scaled.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        seq, fitness = line.split(';')\n",
    "        seqs.append(seq.strip())\n",
    "        ordered_seq_neg.append(seq.strip())\n",
    "        seq_fitness_dict[seq] = float(fitness.strip())\n",
    "with open(f'results/latent_directed_evo/cas12f/cas12f_positive_sequences_with_fitness_scaled.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        seq, fitness = line.split(';')\n",
    "        seqs.append(seq.strip())\n",
    "        ordered_seq_pos.append(seq.strip())\n",
    "        seq_fitness_dict[seq] = float(fitness.strip())\n",
    "ordered_seq_pos_set = set(ordered_seq_pos)\n",
    "ordered_seq_neg_set = set(ordered_seq_neg)\n",
    "\n",
    "print(\"# variants: {}\".format(len(seqs)))\n",
    "NUM_BATCHES = len(seqs) // BATCH_SIZE + 1\n",
    "seqs = seqs + [seqs[-1], ] * (NUM_BATCHES * BATCH_SIZE - len(seqs))\n",
    "seq_len = len(seqs[0])\n",
    "print(\"# BATCHES: {}, seq_len: {}\".format(NUM_BATCHES, seq_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward PF-ODE: data -> latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_embedding_dict = {}\n",
    "for b in tqdm(range(NUM_BATCHES)):\n",
    "    data_dicts = []\n",
    "    for i, seq in enumerate(seqs[b*BATCH_SIZE:(b+1)*BATCH_SIZE]):\n",
    "        protoken_file = 'results/latent_directed_evo/cas12f/stage_1/generator_inputs/af3_model.pkl'\n",
    "\n",
    "        with open(protoken_file, 'rb') as f:\n",
    "            data = pkl.load(f)\n",
    "        \n",
    "        aatypes = resname_to_aatype_index(seq.strip())\n",
    "        seq_len = data['seq_len']\n",
    "        embedding = np.concatenate(\n",
    "            [protoken_emb[data['protokens'].astype(np.int32)], \n",
    "             aatype_emb[aatypes]], axis=-1\n",
    "        )\n",
    "        embedding = np.pad(embedding, ((0, NRES - seq_len), (0,0)))\n",
    "        data_dicts.append(\n",
    "            {'embedding': embedding, \n",
    "             'seq_mask': np.pad(data['seq_mask'], (0, NRES - seq_len)).astype(np.bool_), \n",
    "             'residue_index': np.pad(data['residue_index'], (0, NRES - seq_len)).astype(np.int32),}\n",
    "        )\n",
    "        \n",
    "    data_dict = {k: np.stack([d[k] for d in data_dicts], axis=0) for k in data_dicts[0].keys()}\n",
    "\n",
    "    ### for pmap: reshape inputs\n",
    "    reshape_func = lambda x:x.reshape(NDEVICES, x.shape[0]//NDEVICES, *x.shape[1:])\n",
    "    data_dict = jax.tree_util.tree_map(reshape_func, data_dict)\n",
    "    \n",
    "    ### forward ODE: data->Gaussian \n",
    "    x0 = data_dict['embedding']\n",
    "    xT = pjit_solve_ode(0, scheduler.num_timesteps, 1.0, x0, data_dict['seq_mask'], data_dict['residue_index'])\n",
    "    xT_np = np.array(xT).reshape(BATCH_SIZE, NRES, DIM_EMB)[:, :seq_len, :]\n",
    "    \n",
    "    seq_embedding_dict.update(\n",
    "        {seq: emb for seq, emb in zip(seqs[b*BATCH_SIZE:(b+1)*BATCH_SIZE], xT_np)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/latent_directed_evo/cas12f/seq_emb_dict.pkl', 'wb') as f:\n",
    "    pkl.dump(seq_embedding_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Directed Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "NROUND = 10\n",
    "N_SAMPLE_PER_ROUND = 10\n",
    "N_TOP_SEQS = 10\n",
    "f_result = open(f'./results/latent_directed_evo/cas12f/simulation_result.csv', 'w')\n",
    "f_result.writelines('simulation_num, round_num, median_activity_scaled,top_activity_scaled,activity_binary_percentage\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_embedding(x, max_order=2):\n",
    "    cent_rep = np.mean(x, axis=-2)\n",
    "    reps = [cent_rep]\n",
    "    for o in range(2, max_order+1):\n",
    "        reps.append(np.mean((x - cent_rep[None, :]) ** o,  axis=-2))\n",
    "    return np.array(reps).reshape(-1)\n",
    "\n",
    "seq_rep_dict = {k: convert_embedding(v) for k, v in seq_embedding_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start simulation \n",
    "random_seeds = np.arange(1, 11)\n",
    "top_fitness_scaled = []\n",
    "median_fitness_scaled = []\n",
    "top_n_pos_rate = []\n",
    "\n",
    "for simulation_round, seed in tqdm(enumerate(random_seeds)):\n",
    "    top_fitness_scaled_seed = []\n",
    "    median_fitness_scaled_seed = []\n",
    "    top_n_pos_rate_seed = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    all_seqs = ordered_seq_neg + ordered_seq_pos\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    sample_ids = set([int(_) for _ in np.random.randint(0, len(all_seqs), N_SAMPLE_PER_ROUND)])\n",
    "    select_seqs = [all_seqs[i] for i in sample_ids]\n",
    "    for r in range(NROUND):\n",
    "        X_train += [seq_rep_dict[s] for s in select_seqs]\n",
    "        y_train += [seq_fitness_dict[s] for s in select_seqs]\n",
    "        # print(f'{r}: {len(select_seqs)}, {len(X_train)}, {len(sample_ids)}')\n",
    "        \n",
    "        x_scale, x_shift = np.std(X_train), np.mean(X_train)\n",
    "        y_scale, y_shift = np.std(y_train), np.mean(y_train)\n",
    "\n",
    "        X_train_ = (np.array(X_train) - x_shift) / x_scale \n",
    "        y_train_ = (np.array(y_train) - y_shift) / y_scale\n",
    "        \n",
    "        model = RandomForestRegressor(n_estimators=100, criterion='friedman_mse', max_depth=None, min_samples_split=2,\n",
    "                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0,\n",
    "                            max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False,\n",
    "                            n_jobs=16, random_state=1, verbose=0, warm_start=False, ccp_alpha=0.0,\n",
    "                            max_samples=None)\n",
    "        model.fit(X_train_, y_train_)\n",
    "        predict_y = model.predict((np.array([seq_rep_dict[s] for s in all_seqs]) - x_shift) / x_scale) * y_scale + y_shift\n",
    "        \n",
    "        top_n_idx = [int(x) for x in np.argsort(predict_y)[::-1]]\n",
    "        top_n_seqs = [all_seqs[top_n_idx[i]] for i in range(N_TOP_SEQS)]\n",
    "\n",
    "        top_n_pos_rate_seed.append(np.sum([1 if s in ordered_seq_pos_set else 0 for s in top_n_seqs]) / N_SAMPLE_PER_ROUND)\n",
    "        median_fitness_scaled_seed.append(np.median([seq_fitness_dict[s] for s in top_n_seqs]))\n",
    "        top_fitness_scaled_seed.append(seq_fitness_dict[top_n_seqs[0]])\n",
    "        \n",
    "        f_result.writelines(f'{simulation_round+1},{r+1},{median_fitness_scaled_seed[-1]},{top_fitness_scaled_seed[-1]},{top_n_pos_rate_seed[-1]}\\n')\n",
    "        \n",
    "        count = 0\n",
    "        select_ids = []\n",
    "        select_seqs = []\n",
    "        for idx in top_n_idx:\n",
    "            if not idx in sample_ids:\n",
    "                count += 1\n",
    "                select_ids.append(idx)\n",
    "                select_seqs.append(all_seqs[idx])\n",
    "            if count == N_SAMPLE_PER_ROUND:\n",
    "                break \n",
    "            \n",
    "        sample_ids = sample_ids | set([int(_) for _ in select_ids])\n",
    "        \n",
    "    top_n_pos_rate.append(top_n_pos_rate_seed)\n",
    "    top_fitness_scaled.append(top_fitness_scaled_seed)\n",
    "    median_fitness_scaled.append(median_fitness_scaled_seed)\n",
    "    \n",
    "f_result.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### high activity candidate rate in top-n\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,3), dpi=300)\n",
    "ax.plot(np.arange(NROUND),\n",
    "        np.mean(top_n_pos_rate, axis=0), marker='o')\n",
    "ax.errorbar(\n",
    "    x = np.arange(NROUND),\n",
    "    y = np.mean(top_n_pos_rate, axis=0),\n",
    "    yerr = np.std(top_n_pos_rate, axis=0), marker='o', alpha=0.5, \n",
    "    capsize = 5.0, )\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('High Activity Candidate Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### top-1 fitness\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=300)\n",
    "ax.plot(np.arange(NROUND),\n",
    "        np.mean(top_fitness_scaled, axis=0), marker='o')\n",
    "ax.errorbar(\n",
    "    x = np.arange(NROUND),\n",
    "    y = np.mean(top_fitness_scaled, axis=0),\n",
    "    yerr = np.std(top_fitness_scaled, axis=0), marker='o', alpha=0.5, \n",
    "    capsize = 5.0, )\n",
    "\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Top-1 Fitness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### meadian fitness top-1\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3), dpi=300)\n",
    "ax.plot(np.arange(NROUND),\n",
    "        np.mean(median_fitness_scaled, axis=0), marker='o')\n",
    "ax.errorbar(\n",
    "    x = np.arange(NROUND),\n",
    "    y = np.mean(median_fitness_scaled, axis=0),\n",
    "    yerr = np.std(median_fitness_scaled, axis=0), marker='o', alpha=0.5, \n",
    "    capsize = 5.0, )\n",
    "\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Top-n Median Fitness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zqk-protoken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
