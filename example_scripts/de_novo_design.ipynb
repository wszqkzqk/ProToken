{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *De-novo* co-design of protein sequences & structrues with PT-DiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides examples of utilizing PT-DiT, a pre-trained multimodal diffusion model, to co-design protein sequences (represented as amino acids) and structures (represented as ProTokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from functools import partial\n",
    "from src.model.diffusion_transformer import DiffusionTransformer\n",
    "from train.schedulers import GaussianDiffusion\n",
    "import datetime\n",
    "from flax.jax_utils import replicate\n",
    "from functools import reduce\n",
    "\n",
    "from configs.global_config import global_config\n",
    "from configs.dit_config import dit_config\n",
    "global_config.dropout_flag = False \n",
    "\n",
    "### Load embedding \n",
    "with open('../embeddings/protoken_emb.pkl', 'rb') as f:\n",
    "    protoken_emb = jnp.array(pkl.load(f), dtype=jnp.float32)\n",
    "with open('../embeddings/aatype_emb.pkl', 'rb') as f:\n",
    "    aatype_emb = jnp.array(pkl.load(f), dtype=jnp.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define constants for PT-DiT inference:\n",
    "* `NRES`: length of proteins \n",
    "* `NSAMPLE_PER_DEVICE`: the number of proteins in each GPU device\n",
    "* `DIM_EMB`: the size of raw embedding (concatenating ProTokens & amino acids)\n",
    "* `NDEVICES`: the number of available GPU devices\n",
    "* `BATCH_SIZE`: the number of proteins generated in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### constants\n",
    "NRES = 256\n",
    "NSAMPLE_PER_DEVICE = 8\n",
    "DIM_EMB = protoken_emb.shape[-1] + aatype_emb.shape[-1] # 40 # 32 + 8\n",
    "NDEVICES = len(jax.devices())\n",
    "\n",
    "BATCH_SIZE = NSAMPLE_PER_DEVICE * NDEVICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Functional Utils for Pre/Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define functional utils for pre-processing inputs and post-processing outputs, for example:\n",
    "* `protoken_emb_distance_fn`: calculate Euclidean distance between two ProToken embeddings \n",
    "* `aatype_emb_distance_fn`: calculate Euclidean distance between two amino acid embeddings\n",
    "* `aatype_index_to_resname`: convert amino acid indexes into corresponding amino acid symbols\n",
    "* `resname_to_aatype_index`: convert amino acid symbols into corresponding amino acid indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### function utils \n",
    "\n",
    "def split_multiple_rng_keys(rng_key, num_keys):\n",
    "    rng_keys = jax.random.split(rng_key, num_keys + 1)\n",
    "    return rng_keys[:-1], rng_keys[-1]\n",
    "\n",
    "def flatten_list_of_dicts(list_of_dicts):\n",
    "    ### [{a: [1,2,3,4]}] -> [{a:1}, {a:2}, {a:3}, {a:4}]\n",
    "    flattened_lists = [[{k: v[i] for k, v in d.items()} \n",
    "                        for i in range(len(next(iter(d.values()))))] for d in list_of_dicts]\n",
    "    return reduce(lambda x, y: x+y, flattened_lists, [])\n",
    "\n",
    "def protoken_emb_distance_fn(x, y):\n",
    "    x_ = x / (jnp.linalg.norm(x, axis=-1, keepdims=True) + 1e-6)\n",
    "    y_ = y / (jnp.linalg.norm(y, axis=-1, keepdims=True) + 1e-6)\n",
    "    \n",
    "    return -jnp.sum(x_ * y_, axis=-1)\n",
    "\n",
    "def aatype_emb_distance_fn(x, y):\n",
    "    return jnp.sum((x - y) ** 2, axis=-1)\n",
    "\n",
    "def aatype_index_to_resname(aatype_index):\n",
    "    restypes = [\n",
    "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P',\n",
    "        'S', 'T', 'W', 'Y', 'V'\n",
    "    ]\n",
    "    \n",
    "    return \"\".join([restypes[int(i)] for i in aatype_index])\n",
    "\n",
    "def resname_to_aatype_index(resnames):\n",
    "    restypes = [\n",
    "        'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P',\n",
    "        'S', 'T', 'W', 'Y', 'V'\n",
    "    ]\n",
    "    return np.array([restypes.index(a) for a in resnames], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load Model & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load model & params \n",
    "dit_model = DiffusionTransformer(\n",
    "    config=dit_config, global_config=global_config\n",
    ")\n",
    "num_diffusion_timesteps = 500\n",
    "scheduler = GaussianDiffusion(num_diffusion_timesteps=num_diffusion_timesteps)\n",
    "\n",
    "#### rng keys\n",
    "rng_key = jax.random.PRNGKey(8888)\n",
    "np.random.seed(7777)\n",
    "\n",
    "##### load params\n",
    "ckpt_path = '../ckpts/PT_DiT_params_2000000.pkl'\n",
    "with open(ckpt_path, \"rb\") as f:\n",
    "    params = pkl.load(f)\n",
    "    params = jax.tree_util.tree_map(lambda x: jnp.array(x), params)\n",
    "    \n",
    "##### replicate params\n",
    "params = replicate(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define Functional Utils for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define functional utils for PT-DiT inference, for example:\n",
    "* `clamp_x0_fn`: to enable clamping trick introduced at [arXiv:2205.14217v1](https://arxiv.org/abs/2205.14217)\n",
    "* `denoise_step`: sample $p(\\mathbf{z}_{t-1}|\\mathbf{z}_t)$\n",
    "* `q_sample`: sample $p(\\mathbf{z}_t|\\mathbf{z}_0)$\n",
    "* `noise_step`: sample $p(\\mathbf{z}_t | \\mathbf{z}_{t-1})$\n",
    "\n",
    "and define `pjit = pmap + jit` (see `jax` documentations) version of functions to enable fast and batched inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### main inference functions\n",
    "jit_apply_fn = jax.jit(dit_model.apply)\n",
    "infer_protuple = True\n",
    "\n",
    "def clamp_x0_fn(x0):\n",
    "    protoken_indexes = \\\n",
    "                jnp.argmin(protoken_emb_distance_fn(x0[..., None, :protoken_emb.shape[-1]], \n",
    "                                                  protoken_emb.reshape((1,)*(len(x0.shape)-1) + protoken_emb.shape)), axis=-1)\n",
    "    if bool(infer_protuple):\n",
    "        aatype_indexes = \\\n",
    "                jnp.argmin(aatype_emb_distance_fn(x0[..., None, protoken_emb.shape[-1]:], \n",
    "                                                  aatype_emb.reshape((1,)*(len(x0.shape)-1) + aatype_emb.shape)), axis=-1)\n",
    "        return jnp.concatenate([protoken_emb[protoken_indexes], aatype_emb[aatype_indexes]], axis=-1)\n",
    "    else:\n",
    "        return protoken_emb[protoken_indexes]\n",
    "\n",
    "def denoise_step(params, x, seq_mask, t, residue_index, rng_key,\n",
    "                 clamp_x0_fn=None):\n",
    "    t = jnp.full((x.shape[0],), t)\n",
    "    indicator = params['params']['protoken_indicator']\n",
    "    if bool(infer_protuple):\n",
    "        indicator = jnp.concatenate([indicator, params['params']['aatype_indicator']], \n",
    "                                    axis=-1)\n",
    "    eps_prime = jit_apply_fn({'params': params['params']['model']}, x + indicator[None, ...], \n",
    "                             seq_mask, t, tokens_rope_index=residue_index)\n",
    "    mean, variance, log_variance = scheduler.p_mean_variance(x, t, eps_prime, clip=False, clamp_x0_fn=clamp_x0_fn)\n",
    "    rng_key, normal_key = jax.random.split(rng_key)\n",
    "    x = mean + jnp.exp(0.5 * log_variance) * jax.random.normal(normal_key, x.shape)\n",
    "    return x, rng_key\n",
    "\n",
    "def q_sample(x, t, rng_key):\n",
    "    t = jnp.full((x.shape[0], ), t)\n",
    "    rng_key, normal_key = jax.random.split(rng_key)\n",
    "    eps = jax.random.normal(normal_key, x.shape, dtype=jnp.float32)\n",
    "    x_t = scheduler.q_sample(x, t, eps)\n",
    "    return x_t, rng_key\n",
    "\n",
    "def noise_step(x, t, rng_key):\n",
    "    t = jnp.full((x.shape[0], ), t)\n",
    "    rng_key, normal_key = jax.random.split(rng_key)\n",
    "    x = scheduler.q_sample_step(x, t, jax.random.normal(normal_key, x.shape))\n",
    "    return x, rng_key\n",
    "\n",
    "def index_from_embedding(x):\n",
    "    # x: (B, Nres, Nemb)\n",
    "    protoken_indexes = \\\n",
    "        jnp.argmin(protoken_emb_distance_fn(x[..., None, :protoken_emb.shape[-1]], \n",
    "                                            protoken_emb[None, None, ...]), axis=-1)\n",
    "    ret = {'protoken_indexes': protoken_indexes}\n",
    "    if bool(infer_protuple):\n",
    "        aatype_indexes = \\\n",
    "            jnp.argmin(aatype_emb_distance_fn(x[..., None, protoken_emb.shape[-1]:], \n",
    "                                                aatype_emb[None, None, ...]), axis=-1)\n",
    "        ret.update({'aatype_indexes': aatype_indexes})\n",
    "        \n",
    "    return ret            \n",
    "    \n",
    "pjit_denoise_step = jax.pmap(jax.jit(partial(denoise_step, clamp_x0_fn=None)), axis_name=\"i\", \n",
    "                            in_axes=(0, 0, 0, None, 0, 0))\n",
    "pjit_denoise_step_clamped = jax.pmap(jax.jit(partial(denoise_step, clamp_x0_fn=clamp_x0_fn)), axis_name=\"i\", \n",
    "                            in_axes=(0, 0, 0, None, 0, 0))\n",
    "pjit_q_sample = jax.pmap(jax.jit(q_sample), axis_name=\"i\",\n",
    "                            in_axes=(0, None, 0))\n",
    "pjit_noise_step = jax.pmap(jax.jit(noise_step), axis_name=\"i\",\n",
    "                            in_axes=(0, None, 0))\n",
    "pjit_index_from_embedding = jax.pmap(jax.jit(index_from_embedding), axis_name=\"i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. De-novo Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare Main Inference Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the main inference function of PT-DiT, and introduce two hyper-parameters may affect results:\n",
    "* `n_eq_steps`: number of equilibirum steps in predictor-corrector tricks ([ICLR2021](https://openreview.net/forum?id=PxTIG12RRHS)), in general, more `n_eq_steps` lead to higher sampling quality\n",
    "* `phasing_time`: we use phasing in diffusion inference, in first phase (large noise scale), we do not use clamping trick ([arXiv:2205.14217v1](https://arxiv.org/abs/2205.14217)), to encourage diversity; in second phase (small noise scale), we use clamping trick to ensure robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eq_steps = 50 ### more n eq steps -> higher quality\n",
    "phasing_time = 250 ### controls balance between diversity & quality, larger phasing time -> higer quality, lower diversity\n",
    "def run_infer(x, seq_mask, residue_index, rng_keys):\n",
    "    for ti in tqdm(range(num_diffusion_timesteps)):\n",
    "        t = num_diffusion_timesteps - ti\n",
    "        denoise_fn = pjit_denoise_step_clamped if t < phasing_time else pjit_denoise_step\n",
    "        \n",
    "        for eq_step in range(n_eq_steps):\n",
    "            x, rng_keys = denoise_fn(params, x, seq_mask, t, residue_index, rng_keys)\n",
    "            x, rng_keys = pjit_noise_step(x, t, rng_keys)\n",
    "            \n",
    "        x, rng_keys = pjit_denoise_step(params, x, seq_mask, t, residue_index, rng_keys)\n",
    "\n",
    "    ret = {'embedding': x, 'seq_mask': seq_mask, 'residue_index': residue_index}\n",
    "    ret.update(pjit_index_from_embedding(x))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(8888)\n",
    "\n",
    "rng_key, normal_key = jax.random.split(rng_key)\n",
    "x = jax.random.normal(rng_key, shape=(BATCH_SIZE, NRES, DIM_EMB), dtype=jnp.float32)\n",
    "seq_mask = jnp.ones((BATCH_SIZE, NRES), dtype=jnp.bool_)\n",
    "residue_index = jnp.tile(jnp.arange(NRES, dtype=jnp.int32)[None, ...], (BATCH_SIZE, 1))\n",
    "\n",
    "### reshape inputs \n",
    "reshape_func = lambda x:x.reshape(NDEVICES, x.shape[0]//NDEVICES, *x.shape[1:])\n",
    "x, seq_mask, residue_index = jax.tree.map(reshape_func, (x, seq_mask, residue_index))\n",
    "\n",
    "print(x.shape, x.dtype)\n",
    "print(seq_mask.shape, seq_mask.dtype)\n",
    "print(residue_index.shape, residue_index.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_keys, rng_key = split_multiple_rng_keys(rng_key, NDEVICES)\n",
    "\n",
    "ret = run_infer(x, seq_mask, residue_index, rng_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = jax.tree_util.tree_map(lambda x:np.array(x).reshape(-1, *x.shape[2:]).tolist(), ret)\n",
    "with open('results/denovo_design/result.pkl', 'wb') as f:\n",
    "    pkl.dump(ret, f)\n",
    "    \n",
    "ret_ = flatten_list_of_dicts([ret])\n",
    "with open('results/denovo_design/result_flatten.pkl', 'wb') as f:\n",
    "    pkl.dump(ret_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decode Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decode 3D coordinates of protein structures from generated ProTokens (with `decode_structure.py` script), and save designed sequences & structures in .pdb format. pdb files are saved in `results/denovo_design/pdb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '../'\n",
    "\n",
    "os.system(f'''export PYTHONPATH={working_dir}/PROTOKEN\n",
    "          python {working_dir}/PROTOKEN/scripts/decode_structure.py\\\n",
    "                --decoder_config {working_dir}/PROTOKEN/config/decoder.yaml\\\n",
    "                --vq_config {working_dir}/PROTOKEN/config/vq.yaml\\\n",
    "                --input_path results/denovo_design/result_flatten.pkl\\\n",
    "                --output_dir results/denovo_design/pdb\\\n",
    "                --load_ckpt_path {working_dir}/ckpts/protoken_params_100000.pkl\\\n",
    "                --padding_len {NRES}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zqk-protoken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
